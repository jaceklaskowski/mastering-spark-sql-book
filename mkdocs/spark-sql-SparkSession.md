title: SparkSession

# SparkSession -- The Entry Point to Spark SQL

`SparkSession` is the entry point to Spark SQL. It is one of the very first objects you create while developing a Spark SQL application.

As a Spark developer, you create a `SparkSession` using the <<builder, SparkSession.builder>> method (that gives you access to <<spark-sql-SparkSession-Builder.adoc#, Builder API>> that you use to configure the session).

[source, scala]
----
import org.apache.spark.sql.SparkSession
val spark = SparkSession.builder
  .appName("My Spark Application")  // optional and will be autogenerated if not specified
  .master("local[*]")               // only for demo and testing purposes, use spark-submit instead
  .enableHiveSupport()              // self-explanatory, isn't it?
  .config("spark.sql.warehouse.dir", "target/spark-warehouse")
  .withExtensions { extensions =>
    extensions.injectResolutionRule { session =>
      ...
    }
    extensions.injectOptimizerRule { session =>
      ...
    }
  }
  .getOrCreate
----

Once created, `SparkSession` allows for <<createDataFrame, creating a DataFrame>> (based on an RDD or a Scala `Seq`), <<createDataset, creating a Dataset>>, accessing the Spark SQL services (e.g. <<experimental, ExperimentalMethods>>, <<listenerManager, ExecutionListenerManager>>, <<udf, UDFRegistration>>), <<sql, executing a SQL query>>, <<table, loading a table>> and the last but not least accessing <<read, DataFrameReader>> interface to load a dataset of the format of your choice (to some extent).

You can link:spark-sql-SparkSession-Builder.adoc#enableHiveSupport[enable Apache Hive support] with support for an link:spark-sql-hive-metastore.adoc[external Hive metastore].

[NOTE]
====
`spark` object in `spark-shell` (the instance of `SparkSession` that is auto-created) has Hive support enabled.

In order to disable the pre-configured Hive support in the `spark` object, use <<spark-sql-StaticSQLConf.adoc#spark.sql.catalogImplementation, spark.sql.catalogImplementation>> internal configuration property with `in-memory` value (that uses <<spark-sql-InMemoryCatalog.adoc#, InMemoryCatalog>> external catalog instead).

[source, scala]
----
$ spark-shell --conf spark.sql.catalogImplementation=in-memory
----
====

You can have as many `SparkSessions` as you want in a single Spark application. The common use case is to keep relational entities separate logically in <<catalog, catalogs>> per `SparkSession`.

In the end, you stop a `SparkSession` using <<stop, SparkSession.stop>> method.

[source, scala]
----
spark.stop
----

[[methods]]
.SparkSession API (Object and Instance Methods)
[cols="1,2",options="header",width="100%"]
|===
| Method
| Description

| `active`
a| [[active]]

[source, scala]
----
active: SparkSession
----

(*New in 2.4.0*)

| <<builder, builder>>
a|

[source, scala]
----
builder(): Builder
----

Object method to create a <<spark-sql-SparkSession-Builder.adoc#, Builder>> to get the current `SparkSession` instance or create a new one.

| <<catalog, catalog>>
a|

[source, scala]
----
catalog: Catalog
----

Access to the current <<spark-sql-Catalog.adoc#, metadata catalog>> of relational entities, e.g. database(s), tables, functions, table columns, and temporary views.

| <<clearActiveSession, clearActiveSession>>
a|

[source, scala]
----
clearActiveSession(): Unit
----

Object method

| <<clearDefaultSession, clearDefaultSession>>
a|

[source, scala]
----
clearDefaultSession(): Unit
----

Object method

| <<close, close>>
a|

[source, scala]
----
close(): Unit
----

| <<conf, conf>>
a|

[source, scala]
----
conf: RuntimeConfig
----

Access to the current runtime configuration

| <<createDataFrame, createDataFrame>>
a|

[source, scala]
----
createDataFrame(rdd: RDD[_], beanClass: Class[_]): DataFrame
createDataFrame(rowRDD: RDD[Row], schema: StructType): DataFrame
createDataFrame[A <: Product : TypeTag](rdd: RDD[A]): DataFrame
createDataFrame[A <: Product : TypeTag](data: Seq[A]): DataFrame
----

| <<createDataset, createDataset>>
a|

[source, scala]
----
createDataset[T : Encoder](data: RDD[T]): Dataset[T]
createDataset[T : Encoder](data: Seq[T]): Dataset[T]
----

| <<emptyDataFrame, emptyDataFrame>>
a|

[source, scala]
----
emptyDataFrame: DataFrame
----

| <<emptyDataset, emptyDataset>>
a|

[source, scala]
----
emptyDataset[T: Encoder]: Dataset[T]
----

| <<experimental, experimental>>
a|

[source, scala]
----
experimental: ExperimentalMethods
----

Access to the current <<spark-sql-ExperimentalMethods.adoc#, ExperimentalMethods>>

| <<getActiveSession, getActiveSession>>
a|

[source, scala]
----
getActiveSession: Option[SparkSession]
----

Object method

| <<getDefaultSession, getDefaultSession>>
a|

[source, scala]
----
getDefaultSession: Option[SparkSession]
----

Object method

| <<spark-sql-SparkSession-implicits.adoc#, implicits>>
a| [[implicits]]

[source, scala]
----
import spark.implicits._
----

<<spark-sql-SparkSession-implicits.adoc#, Implicits conversions>>

| <<listenerManager, listenerManager>>
a|

[source, scala]
----
listenerManager: ExecutionListenerManager
----

Access to the current <<spark-sql-ExecutionListenerManager.adoc#, ExecutionListenerManager>>

| <<newSession, newSession>>
a|

[source, scala]
----
newSession(): SparkSession
----

Creates a new `SparkSession`

| <<range, range>>
a|

[source, scala]
----
range(end: Long): Dataset[java.lang.Long]
range(start: Long, end: Long): Dataset[java.lang.Long]
range(start: Long, end: Long, step: Long): Dataset[java.lang.Long]
range(start: Long, end: Long, step: Long, numPartitions: Int): Dataset[java.lang.Long]
----

Creates a `Dataset[java.lang.Long]`

| <<read, read>>
a|

[source, scala]
----
read: DataFrameReader
----

Access to the current [DataFrameReader](DataFrameReader.md) to load data from external data sources

| <<sessionState, sessionState>>
a|

[source, scala]
----
sessionState: SessionState
----

Access to the current <<spark-sql-SessionState.adoc#, SessionState>>

Internally, `sessionState` <<spark-sql-SessionState.adoc#clone, clones>> the optional <<parentSessionState, parent SessionState>> (if given when <<creating-instance, creating the SparkSession>>) or <<instantiateSessionState, creates a new SessionState>> using <<spark-sql-BaseSessionStateBuilder.adoc#, BaseSessionStateBuilder>> as defined by <<spark-sql-StaticSQLConf.adoc#spark.sql.catalogImplementation, spark.sql.catalogImplementation>> configuration property:

* *in-memory* (default) for link:spark-sql-SessionStateBuilder.adoc[org.apache.spark.sql.internal.SessionStateBuilder]
* *hive* for link:hive/HiveSessionStateBuilder.adoc[org.apache.spark.sql.hive.HiveSessionStateBuilder]

| <<setActiveSession, setActiveSession>>
a|

[source, scala]
----
setActiveSession(session: SparkSession): Unit
----

Object method

| <<setDefaultSession, setDefaultSession>>
a|

[source, scala]
----
setDefaultSession(session: SparkSession): Unit
----

Object method

| <<sharedState, sharedState>>
a|

[source, scala]
----
sharedState: SharedState
----

Access to the current <<spark-sql-SharedState.adoc#, SharedState>>

| <<sparkContext, sparkContext>>
a|

[source, scala]
----
sparkContext: SparkContext
----

Access to the underlying `SparkContext`

| <<sql, sql>>
a|

[source, scala]
----
sql(sqlText: String): DataFrame
----

"Executes" a SQL query

| `sessionState`
a| [[sessionState]]

[source, scala]
----
sessionState: SessionState
----

link:spark-sql-SessionState.adoc[SessionState]

| `sqlContext`
a| [[sqlContext]]

[source, scala]
----
sqlContext: SQLContext
----

Access to the underlying <<spark-sql-SQLContext.adoc#, SQLContext>>

| <<stop, stop>>
a|

[source, scala]
----
stop(): Unit
----

Stops the associated <<sparkContext, SparkContext>>

| <<table, table>>
a|

[source, scala]
----
table(tableName: String): DataFrame
----

Loads data from a table

| <<time, time>>
a|

[source, scala]
----
time[T](f: => T): T
----

Executes a code block and prints out (to standard output) the time taken to execute it

| <<udf, udf>>
a|

[source, scala]
----
udf: UDFRegistration
----

Access to the current <<spark-sql-UDFRegistration.adoc#, UDFRegistration>>

| <<version, version>>
a|

[source, scala]
----
version: String
----

Returns the version of Apache Spark
|===

NOTE: <<baseRelationToDataFrame, baseRelationToDataFrame>> acts as a mechanism to plug `BaseRelation` object hierarchy in into link:spark-sql-LogicalPlan.adoc[LogicalPlan] object hierarchy that `SparkSession` uses to bridge them.

=== [[builder]] Creating SparkSession Using Builder Pattern -- `builder` Object Method

[source, scala]
----
builder(): Builder
----

`builder` creates a new link:spark-sql-SparkSession-Builder.adoc[Builder] that you use to build a fully-configured `SparkSession` using a _fluent API_.

[source, scala]
----
import org.apache.spark.sql.SparkSession
val builder = SparkSession.builder
----

TIP: Read about https://en.wikipedia.org/wiki/Fluent_interface[Fluent interface] design pattern in Wikipedia, the free encyclopedia.

=== [[version]] Accessing Version of Spark -- `version` Method

[source, scala]
----
version: String
----

`version` returns the version of Apache Spark in use.

Internally, `version` uses `spark.SPARK_VERSION` value that is the `version` property in `spark-version-info.properties` properties file on CLASSPATH.

=== [[emptyDataset]] Creating Empty Dataset (Given Encoder) -- `emptyDataset` Operator

[source, scala]
----
emptyDataset[T: Encoder]: Dataset[T]
----

`emptyDataset` creates an empty link:spark-sql-Dataset.adoc[Dataset] (assuming that future records being of type `T`).

[source, scala]
----
scala> val strings = spark.emptyDataset[String]
strings: org.apache.spark.sql.Dataset[String] = [value: string]

scala> strings.printSchema
root
 |-- value: string (nullable = true)
----

`emptyDataset` creates a  link:spark-sql-LogicalPlan-LocalRelation.adoc[`LocalRelation` logical query plan].

=== [[createDataset]] Creating Dataset from Local Collections or RDDs -- `createDataset` Methods

[source, scala]
----
createDataset[T : Encoder](data: RDD[T]): Dataset[T]
createDataset[T : Encoder](data: Seq[T]): Dataset[T]
----

`createDataset` creates a link:spark-sql-Dataset.adoc[Dataset] from a local Scala collection, i.e. `Seq[T]`, Java's `List[T]`, or a distributed `RDD[T]`.

[source, scala]
----
scala> val one = spark.createDataset(Seq(1))
one: org.apache.spark.sql.Dataset[Int] = [value: int]

scala> one.show
+-----+
|value|
+-----+
|    1|
+-----+
----

`createDataset` creates a link:spark-sql-LogicalPlan-LocalRelation.adoc[LocalRelation] (for the input `data` collection) or link:spark-sql-LogicalPlan-LogicalRDD.adoc[LogicalRDD] (for the input `RDD[T]`) logical operators.

[TIP]
====
You may want to consider <<spark-sql-SparkSession-implicits.adoc#, implicits>> object and `toDS` method instead.

[source, scala]
----
val spark: SparkSession = ...
import spark.implicits._

scala> val one = Seq(1).toDS
one: org.apache.spark.sql.Dataset[Int] = [value: int]
----
====

Internally, `createDataset` first looks up the implicit link:spark-sql-ExpressionEncoder.adoc[expression encoder] in scope to access the ``AttributeReference``s (of the link:spark-sql-schema.adoc[schema]).

NOTE: Only unresolved link:spark-sql-ExpressionEncoder.adoc[expression encoders] are currently supported.

The expression encoder is then used to map elements (of the input `Seq[T]`) into a collection of link:spark-sql-InternalRow.adoc[InternalRows]. With the references and rows, `createDataset` returns a link:spark-sql-Dataset.adoc[Dataset] with a link:spark-sql-LogicalPlan-LocalRelation.adoc[`LocalRelation` logical query plan].

=== [[range]] Creating Dataset With Single Long Column -- `range` Operator

[source, scala]
----
range(end: Long): Dataset[java.lang.Long]
range(start: Long, end: Long): Dataset[java.lang.Long]
range(start: Long, end: Long, step: Long): Dataset[java.lang.Long]
range(start: Long, end: Long, step: Long, numPartitions: Int): Dataset[java.lang.Long]
----

`range` family of methods create a link:spark-sql-Dataset.adoc[Dataset] of `Long` numbers.

[source, scala]
----
scala> spark.range(start = 0, end = 4, step = 2, numPartitions = 5).show
+---+
| id|
+---+
|  0|
|  2|
+---+
----

NOTE: The three first variants (that do not specify `numPartitions` explicitly) use link:spark-SparkContext.adoc#defaultParallelism[SparkContext.defaultParallelism] for the number of partitions `numPartitions`.

Internally, `range` creates a new `Dataset[Long]` with `Range` link:spark-sql-LogicalPlan.adoc[logical plan] and `Encoders.LONG` link:spark-sql-Encoder.adoc[encoder].

=== [[emptyDataFrame]]  Creating Empty DataFrame --  `emptyDataFrame` method

[source, scala]
----
emptyDataFrame: DataFrame
----

`emptyDataFrame` creates an empty `DataFrame` (with no rows and columns).

It calls <<createDataFrame, createDataFrame>> with an empty `RDD[Row]` and an empty schema link:spark-sql-StructType.adoc[StructType(Nil)].

=== [[createDataFrame]] Creating DataFrames from Local Collections or RDDs -- `createDataFrame` Method

[source, scala]
----
createDataFrame(rdd: RDD[_], beanClass: Class[_]): DataFrame
createDataFrame(rowRDD: RDD[Row], schema: StructType): DataFrame
createDataFrame[A <: Product : TypeTag](rdd: RDD[A]): DataFrame
createDataFrame[A <: Product : TypeTag](data: Seq[A]): DataFrame
// private[sql]
createDataFrame(rowRDD: RDD[Row], schema: StructType, needsConversion: Boolean): DataFrame
----

`createDataFrame` creates a `DataFrame` using `RDD[Row]` and the input `schema`. It is assumed that the rows in `rowRDD` all match the `schema`.

CAUTION: FIXME

=== [[sql]] Executing SQL Queries (aka SQL Mode) -- `sql` Method

[source, scala]
----
sql(sqlText: String): DataFrame
----

`sql` executes the `sqlText` SQL statement and creates a link:spark-sql-DataFrame.adoc[DataFrame].

[NOTE]
====
`sql` is imported in link:spark-shell.adoc[spark-shell] so you can execute SQL statements as if `sql` were a part of the environment.

```
scala> :imports
 1) import spark.implicits._       (72 terms, 43 are implicit)
 2) import spark.sql               (1 terms)
```
====

```
scala> sql("SHOW TABLES")
res0: org.apache.spark.sql.DataFrame = [tableName: string, isTemporary: boolean]

scala> sql("DROP TABLE IF EXISTS testData")
res1: org.apache.spark.sql.DataFrame = []

// Let's create a table to SHOW it
spark.range(10).write.option("path", "/tmp/test").saveAsTable("testData")

scala> sql("SHOW TABLES").show
+---------+-----------+
|tableName|isTemporary|
+---------+-----------+
| testdata|      false|
+---------+-----------+
```

Internally, `sql` requests the link:spark-sql-SessionState.adoc#sqlParser[current `ParserInterface`] to link:spark-sql-ParserInterface.adoc#parsePlan[execute a SQL query] that gives a link:spark-sql-LogicalPlan.adoc[LogicalPlan].

NOTE: `sql` uses `SessionState` link:spark-sql-SessionState.adoc#sqlParser[to access the current `ParserInterface`].

`sql` then creates a link:spark-sql-DataFrame.adoc[DataFrame] using the current `SparkSession` (itself) and the link:spark-sql-LogicalPlan.adoc[LogicalPlan].

[TIP]
====
link:spark-sql-spark-sql.adoc[spark-sql] is the main SQL environment in Spark to work with pure SQL statements (where you do not have to use Scala to execute them).

```
spark-sql> show databases;
default
Time taken: 0.028 seconds, Fetched 1 row(s)
```
====

=== [[udf]] Accessing UDFRegistration -- `udf` Attribute

[source, scala]
----
udf: UDFRegistration
----

`udf` attribute gives access to link:spark-sql-UDFRegistration.adoc[UDFRegistration] that allows registering link:spark-sql-udfs.adoc[user-defined functions] for SQL-based queries.

[source, scala]
----
val spark: SparkSession = ...
spark.udf.register("myUpper", (s: String) => s.toUpperCase)

val strs = ('a' to 'c').map(_.toString).toDS
strs.registerTempTable("strs")

scala> sql("SELECT *, myUpper(value) UPPER FROM strs").show
+-----+-----+
|value|UPPER|
+-----+-----+
|    a|    A|
|    b|    B|
|    c|    C|
+-----+-----+
----

Internally, it is simply an alias for link:spark-sql-SessionState.adoc#udfRegistration[SessionState.udfRegistration].

=== [[table]] Loading Data From Table -- `table` Method

[source, scala]
----
table(tableName: String): DataFrame // <1>
// private[sql]
table(tableIdent: TableIdentifier): DataFrame
----
<1> Parses `tableName` to a `TableIdentifier` and calls the other `table`

`table` creates a link:spark-sql-DataFrame.adoc[DataFrame] (wrapper) from the input `tableName` table (but only if link:spark-sql-SessionCatalog.adoc#lookupRelation[available in the session catalog]).

[source, scala]
----
scala> spark.catalog.tableExists("t1")
res1: Boolean = true

// t1 exists in the catalog
// let's load it
val t1 = spark.table("t1")
----

=== [[catalog]] Accessing Metastore -- `catalog` Attribute

[source, scala]
----
catalog: Catalog
----

`catalog` attribute is a (lazy) interface to the current metastore, i.e. link:spark-sql-Catalog.adoc[data catalog] (of relational entities like databases, tables, functions, table columns, and views).

TIP: All methods in `Catalog` return `Datasets`.

[source, scala]
----
scala> spark.catalog.listTables.show
+------------------+--------+-----------+---------+-----------+
|              name|database|description|tableType|isTemporary|
+------------------+--------+-----------+---------+-----------+
|my_permanent_table| default|       null|  MANAGED|      false|
|              strs|    null|       null|TEMPORARY|       true|
+------------------+--------+-----------+---------+-----------+
----

Internally, `catalog` creates a link:spark-sql-CatalogImpl.adoc[CatalogImpl] (that uses the current `SparkSession`).

=== [[read]] Accessing DataFrameReader -- `read` method

[source, scala]
----
read: DataFrameReader
----

`read` method returns a [DataFrameReader](DataFrameReader.md) that is used to read data from external storage systems and load it into a `DataFrame`.

[source, scala]
----
val spark: SparkSession = // create instance
val dfReader: DataFrameReader = spark.read
----

=== [[conf]] Getting Runtime Configuration -- `conf` Attribute

[source, scala]
----
conf: RuntimeConfig
----

`conf` returns the current <<spark-sql-RuntimeConfig.adoc#, RuntimeConfig>>.

Internally, `conf` creates a <<spark-sql-RuntimeConfig.adoc#creating-instance, RuntimeConfig>> (when requested the very first time and cached afterwards) with the <<spark-sql-SessionState.adoc#conf, SQLConf>> of the <<sessionState, SessionState>>.

=== [[readStream]] `readStream` method

[source, scala]
----
readStream: DataStreamReader
----

`readStream` returns a new link:spark-sql-streaming-DataStreamReader.adoc[DataStreamReader].

=== [[streams]] `streams` Attribute

[source, scala]
----
streams: StreamingQueryManager
----

`streams` attribute gives access to link:spark-sql-streaming-StreamingQueryManager.adoc[StreamingQueryManager] (through link:spark-sql-SessionState.adoc#streamingQueryManager[SessionState]).

[source, scala]
----
val spark: SparkSession = ...
spark.streams.active.foreach(println)
----

=== [[experimentalMethods]] `experimentalMethods` Attribute

[source, scala]
----
experimental: ExperimentalMethods
----

`experimentalMethods` is an extension point with link:spark-sql-ExperimentalMethods.adoc[ExperimentalMethods] that is a per-session collection of extra strategies and ``Rule[LogicalPlan]``s.

NOTE: `experimental` is used in link:spark-sql-SparkPlanner.adoc[SparkPlanner] and link:spark-sql-SparkOptimizer.adoc[SparkOptimizer]. Hive and link:spark-structured-streaming.adoc[Structured Streaming] use it for their own extra strategies and optimization rules.

=== [[newSession]] Creating SparkSession Instance -- `newSession` method

[source, scala]
----
newSession(): SparkSession
----

`newSession` creates (starts) a new `SparkSession` (with the current link:spark-SparkContext.adoc[SparkContext] and link:spark-sql-SharedState.adoc[SharedState]).

[source, scala]
----
scala> val newSession = spark.newSession
newSession: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@122f58a
----

=== [[stop]] Stopping SparkSession -- `stop` Method

[source, scala]
----
stop(): Unit
----

`stop` stops the `SparkSession`, i.e. link:spark-SparkContext.adoc#stop[stops the underlying `SparkContext`].

=== [[baseRelationToDataFrame]] Create DataFrame for BaseRelation -- `baseRelationToDataFrame` Method

[source, scala]
----
baseRelationToDataFrame(
  baseRelation: BaseRelation): DataFrame
----

Internally, `baseRelationToDataFrame` creates a link:spark-sql-DataFrame.adoc[DataFrame] from the input link:spark-sql-BaseRelation.adoc[BaseRelation] wrapped inside link:spark-sql-LogicalPlan-LogicalRelation.adoc[LogicalRelation].

NOTE: link:spark-sql-LogicalPlan-LogicalRelation.adoc[LogicalRelation] is an logical plan adapter for `BaseRelation` (so `BaseRelation` can be part of a link:spark-sql-LogicalPlan.adoc[logical plan]).

[NOTE]
====
`baseRelationToDataFrame` is used when:

* `DataFrameReader` [loads data from a data source that supports multiple paths](DataFrameReader.md#load)
* `DataFrameReader` [loads data from an external table using JDBC](DataFrameReader.md#jdbc)
* `TextInputCSVDataSource` creates a base `Dataset` (of Strings)
* `TextInputJsonDataSource` creates a base `Dataset` (of Strings)
====

=== [[instantiateSessionState]] Creating SessionState Instance -- `instantiateSessionState` Internal Method

[source, scala]
----
instantiateSessionState(className: String, sparkSession: SparkSession): SessionState
----

`instantiateSessionState` finds the `className` that is then used to link:spark-sql-BaseSessionStateBuilder.adoc#creating-instance[create] and link:spark-sql-BaseSessionStateBuilder.adoc#build[build] a `BaseSessionStateBuilder`.

`instantiateSessionState` may report an `IllegalArgumentException` while instantiating the class of a `SessionState`:

```
Error while instantiating '[className]'
```

NOTE: `instantiateSessionState` is used exclusively when `SparkSession` is requested for <<sessionState, SessionState>> per link:spark-sql-StaticSQLConf.adoc#spark.sql.catalogImplementation[spark.sql.catalogImplementation] configuration property (and one is not available yet).

=== [[sessionStateClassName]] `sessionStateClassName` Internal Method

[source, scala]
----
sessionStateClassName(
  conf: SparkConf): String
----

`sessionStateClassName` gives the name of the class of the link:spark-sql-SessionState.adoc[SessionState] per link:spark-sql-StaticSQLConf.adoc#spark.sql.catalogImplementation[spark.sql.catalogImplementation], i.e.

* link:hive/HiveSessionStateBuilder.adoc[org.apache.spark.sql.hive.HiveSessionStateBuilder] for `hive`
* link:spark-sql-SessionStateBuilder.adoc[org.apache.spark.sql.internal.SessionStateBuilder] for `in-memory`

NOTE: `sessionStateClassName` is used exclusively when `SparkSession` is requested for the <<sessionState, SessionState>> (and one is not available yet).

=== [[internalCreateDataFrame]] Creating DataFrame From RDD Of Internal Binary Rows and Schema -- `internalCreateDataFrame` Internal Method

[source, scala]
----
internalCreateDataFrame(
  catalystRows: RDD[InternalRow],
  schema: StructType,
  isStreaming: Boolean = false): DataFrame
----

`internalCreateDataFrame` creates a link:spark-sql-Dataset.adoc#ofRows[DataFrame] with a link:spark-sql-LogicalPlan-LogicalRDD.adoc#creating-instance[LogicalRDD].

`internalCreateDataFrame` is used when:

* `DataFrameReader` is requested to create a DataFrame from Dataset of [JSONs](DataFrameReader.md#json) or [CSVs](DataFrameReader.md#csv)

* `SparkSession` is requested to <<createDataFrame, create a DataFrame from RDD of rows>>

* `InsertIntoDataSourceCommand` logical command is <<spark-sql-LogicalPlan-InsertIntoDataSourceCommand.adoc#run, executed>>

=== [[creating-instance]] Creating SparkSession Instance

`SparkSession` takes the following when created:

* [[sparkContext]] Spark Core's `SparkContext`
* [[existingSharedState]] Optional <<spark-sql-SharedState.adoc#, SharedState>>
* [[parentSessionState]] Optional <<spark-sql-SessionState.adoc#, SessionState>>
* [[extensions]] <<spark-sql-SparkSessionExtensions.adoc#, SparkSessionExtensions>>

=== [[clearActiveSession]] `clearActiveSession` Object Method

[source, scala]
----
clearActiveSession(): Unit
----

`clearActiveSession`...FIXME

=== [[clearDefaultSession]] `clearDefaultSession` Object Method

[source, scala]
----
clearDefaultSession(): Unit
----

`clearDefaultSession`...FIXME

=== [[experimental]] Accessing ExperimentalMethods -- `experimental` Method

[source, scala]
----
experimental: ExperimentalMethods
----

`experimental`...FIXME

=== [[getActiveSession]] `getActiveSession` Object Method

[source, scala]
----
getActiveSession: Option[SparkSession]
----

`getActiveSession`...FIXME

=== [[getDefaultSession]] `getDefaultSession` Object Method

[source, scala]
----
getDefaultSession: Option[SparkSession]
----

`getDefaultSession`...FIXME

=== [[listenerManager]] Accessing ExecutionListenerManager -- `listenerManager` Method

[source, scala]
----
listenerManager: ExecutionListenerManager
----

`listenerManager`...FIXME

=== [[setActiveSession]] `setActiveSession` Object Method

[source, scala]
----
setActiveSession(session: SparkSession): Unit
----

`setActiveSession`...FIXME

=== [[setDefaultSession]] `setDefaultSession` Object Method

[source, scala]
----
setDefaultSession(session: SparkSession): Unit
----

`setDefaultSession`...FIXME

=== [[sharedState]] Accessing SharedState -- `sharedState` Method

[source, scala]
----
sharedState: SharedState
----

`sharedState`...FIXME

=== [[time]] Measuring Duration of Executing Code Block -- `time` Method

[source, scala]
----
time[T](f: => T): T
----

`time`...FIXME
