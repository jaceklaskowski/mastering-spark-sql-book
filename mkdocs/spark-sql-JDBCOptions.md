title: Options

# JDBCOptions -- JDBC Data Source Options

`JDBCOptions` represents the <<options, options>> of the <<spark-sql-jdbc.adoc#, JDBC data source>>.

[[options]]
.Options for JDBC Data Source
[cols="1m,1,2",options="header",width="100%",separator="!"]
|===
! Option / Key
! Default Value
! Description

! batchsize
! `1000`
! [[batchsize]]

The minimum value is `1`

Used exclusively when `JdbcRelationProvider` is requested to <<spark-sql-JdbcRelationProvider.adoc#createRelation-CreatableRelationProvider, write the rows of a structured query (a DataFrame) to a table>> through `JdbcUtils` helper object and its <<spark-sql-JdbcUtils.adoc#saveTable, saveTable>>.

! createTableColumnTypes
!
! [[createTableColumnTypes]]

Used exclusively when `JdbcRelationProvider` is requested to <<spark-sql-JdbcRelationProvider.adoc#createRelation-CreatableRelationProvider, write the rows of a structured query (a DataFrame) to a table>> through `JdbcUtils` helper object and its <<spark-sql-JdbcUtils.adoc#createTable, createTable>>.

! `createTableOptions`
! Empty string
! [[createTableOptions]]

Used exclusively when `JdbcRelationProvider` is requested to <<spark-sql-JdbcRelationProvider.adoc#createRelation-CreatableRelationProvider, write the rows of a structured query (a DataFrame) to a table>> through `JdbcUtils` helper object and its <<spark-sql-JdbcUtils.adoc#createTable, createTable>>.

! `customSchema`
! (undefined)
a! [[customSchema]] Specifies the custom data types of the read schema (that is used at link:spark-sql-DataFrameReader.adoc#jdbc[load time])

`customSchema` is a comma-separated list of field definitions with column names and their link:spark-sql-DataType.adoc[data types] in a canonical SQL representation, e.g. `id DECIMAL(38, 0), name STRING`.

`customSchema` defines the data types of the columns that will override the data types inferred from the table schema and follows the following pattern:

```
colTypeList
    : colType (',' colType)*
    ;

colType
    : identifier dataType (COMMENT STRING)?
    ;

dataType
    : complex=ARRAY '<' dataType '>'                            #complexDataType
    | complex=MAP '<' dataType ',' dataType '>'                 #complexDataType
    | complex=STRUCT ('<' complexColTypeList? '>' | NEQ)        #complexDataType
    | identifier ('(' INTEGER_VALUE (',' INTEGER_VALUE)* ')')?  #primitiveDataType
    ;
```

Used exclusively when `JDBCRelation` is requested for the <<spark-sql-JDBCRelation.adoc#schema, schema>>.

! `dbtable`
!
a! [[dbtable]] (*required*)

Used when:

* `JDBCRDD` is requested to <<spark-sql-JDBCRDD.adoc#resolveTable, resolveTable>> (when `JDBCRelation` is requested for the <<spark-sql-JDBCRelation.adoc#schema, schema>>) and <<spark-sql-JDBCRelation.adoc#compute, compute a partition>>

* `JDBCRelation` is requested to <<spark-sql-JDBCRelation.adoc#insert, insert or overwrite data>> and for the <<spark-sql-JDBCRelation.adoc#toString, human-friendly text representation>>

* `JdbcRelationProvider` is requested to <<spark-sql-JdbcRelationProvider.adoc#createRelation-CreatableRelationProvider, write the rows of a structured query (a DataFrame) to a table>>

* `JdbcUtils` is requested to <<spark-sql-JdbcUtils.adoc#tableExists, tableExists>>, <<spark-sql-JdbcUtils.adoc#truncateTable, truncateTable>>, <<spark-sql-JdbcUtils.adoc#getSchemaOption, getSchemaOption>>, <<spark-sql-JdbcUtils.adoc#saveTable, saveTable>> and <<spark-sql-JdbcUtils.adoc#createTable, createTable>>

* `JDBCOptions` is <<creating-instance, created>> (with the input parameters for the <<url, url>> and <<dbtable, dbtable>> options)

* `DataFrameReader` is requested to <<spark-sql-DataFrameReader.adoc#jdbc, load data from external table using JDBC data source>> (using `DataFrameReader.jdbc` method with the input parameters for the <<url, url>> and <<dbtable, dbtable>> options)

! `driver`
!
a! [[driver]][[driverClass]] (*recommended*) Class name of the JDBC driver to use

Used exclusively when `JDBCOptions` is <<creating-instance, created>>. When the `driver` option is defined, the JDBC driver class will get registered with Java's https://docs.oracle.com/javase/8/docs/api/java/sql/DriverManager.html[java.sql.DriverManager].

NOTE: `driver` takes precedence over the class name of the driver for the <<url, url>> option.

After the JDBC driver class was registered, the driver class is used exclusively when `JdbcUtils` helper object is requested to <<spark-sql-JdbcUtils.adoc#createConnectionFactory, createConnectionFactory>>.

! `fetchsize`
! `0`
! [[fetchsize]] Hint to the JDBC driver as to the number of rows that should be fetched from the database when more rows are needed for `ResultSet` objects generated by a `Statement`

The minimum value is `0` (which tells the JDBC driver to do the estimates)

Used exclusively when `JDBCRDD` is requested to <<spark-sql-JDBCRDD.adoc#compute, compute a partition>>.

! `isolationLevel`
! `READ_UNCOMMITTED`
a! [[isolationLevel]] One of the following:

* NONE
* READ_UNCOMMITTED
* READ_COMMITTED
* REPEATABLE_READ
* SERIALIZABLE

Used exclusively when `JdbcUtils` is requested to <<spark-sql-JdbcUtils.adoc#saveTable, saveTable>>.

! `lowerBound`
!
! [[lowerBound]] Lower bound of partition column

Used exclusively when `JdbcRelationProvider` is requested to <<spark-sql-JdbcRelationProvider.adoc#createRelation-RelationProvider, create a BaseRelation>> for reading

! `numPartitions`
!
a! [[numPartitions]] Number of partitions to use for loading or saving data

Used when:

* `JdbcRelationProvider` is requested to <<spark-sql-JdbcRelationProvider.adoc#createRelation-RelationProvider, loading data from a table using JDBC>>

* `JdbcUtils` is requested to <<spark-sql-JdbcUtils.adoc#saveTable, saveTable>>

! `partitionColumn`
!
! [[partitionColumn]] Name of the column used to partition dataset (using a `JDBCPartitioningInfo`).

Used exclusively when `JdbcRelationProvider` is requested to <<spark-sql-JdbcRelationProvider.adoc#createRelation-RelationProvider, create a BaseRelation>> for reading (with proper `JDBCPartitions` with `WHERE` clause)

When defined, the <<lowerBound, lowerBound>>, <<upperBound, upperBound>> and <<numPartitions, numPartitions>> options are also required.

When undefined, <<lowerBound, lowerBound>> and <<upperBound, upperBound>> have to be undefined.

! `truncate`
! `false`
! [[truncate]][[isTruncate]] (used only for writing) Enables table truncation

Used exclusively when `JdbcRelationProvider` is requested to <<spark-sql-JdbcRelationProvider.adoc#createRelation-CreatableRelationProvider, write the rows of a structured query (a DataFrame) to a table>>

! `sessionInitStatement`
!
! [[sessionInitStatement]] A generic SQL statement (or PL/SQL block) executed before reading a table/query

Used exclusively when `JDBCRDD` is requested to <<spark-sql-JDBCRDD.adoc#compute, compute a partition>>.

! `upperBound`
!
! [[upperBound]] Upper bound of the partition column

Used exclusively when `JdbcRelationProvider` is requested to <<spark-sql-JdbcRelationProvider.adoc#createRelation-RelationProvider, create a BaseRelation>> for reading

! `url`
!
! [[url]] (*required*) A JDBC URL to use to connect to a database
|===

NOTE: The <<options, options>> are case-insensitive.

`JDBCOptions` is <<creating-instance, created>> when:

* `DataFrameReader` is requested to <<spark-sql-DataFrameReader.adoc#jdbc, load data from an external table using JDBC>> (and create a `DataFrame` to represent the process of loading the data)

* `JdbcRelationProvider` is requested to create a `BaseRelation` (as a <<spark-sql-JdbcRelationProvider.adoc#createRelation-RelationProvider, RelationProvider>> for loading and a <<spark-sql-JdbcRelationProvider.adoc#createRelation-CreatableRelationProvider, CreatableRelationProvider>> for writing)

=== [[creating-instance]] Creating JDBCOptions Instance

`JDBCOptions` takes the following when created:

* JDBC URL
* [[table]] Name of the table
* [[parameters]] Case-insensitive configuration parameters (i.e. `Map[String, String]`)

The input `URL` and <<table, table>> are set as the current <<url, url>> and <<dbtable, dbtable>> options (overriding the values in the input <<parameters, parameters>> if defined).

=== [[asProperties]] Converting Parameters (Options) to Java Properties -- `asProperties` Property

[source, scala]
----
asProperties: Properties
----

`asProperties`...FIXME

[NOTE]
====
`asProperties` is used when:

* `JDBCRDD` is requested to link:spark-sql-JDBCRDD.adoc#compute[compute a partition] (that requests a `JdbcDialect` to link:spark-sql-JdbcDialect.adoc#beforeFetch[beforeFetch])

* `JDBCRelation` is requested to link:spark-sql-JDBCRelation.adoc#insert[insert a data (from a DataFrame) to a table]
====

=== [[asConnectionProperties]] `asConnectionProperties` Property

[source, scala]
----
asConnectionProperties: Properties
----

`asConnectionProperties`...FIXME

NOTE: `asConnectionProperties` is used exclusively when `JdbcUtils` is requested to link:spark-sql-JdbcUtils.adoc#createConnectionFactory[createConnectionFactory]
