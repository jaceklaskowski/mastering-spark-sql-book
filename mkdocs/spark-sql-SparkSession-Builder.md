title: Builder

# Builder -- Building SparkSession using Fluent API

`Builder` is the <<methods, fluent API>> to create a <<spark-sql-SparkSession.adoc#, SparkSession>>.

[[methods]]
.Builder API
[cols="1,2",options="header",width="100%"]
|===
| Method
| Description

| <<appName, appName>>
a|

[source, scala]
----
appName(name: String): Builder
----

| <<config, config>>
a|

[source, scala]
----
config(conf: SparkConf): Builder
config(key: String, value: Boolean): Builder
config(key: String, value: Double): Builder
config(key: String, value: Long): Builder
config(key: String, value: String): Builder
----

| <<enableHiveSupport, enableHiveSupport>>
a| Enables Hive support

[source, scala]
----
enableHiveSupport(): Builder
----

| <<getOrCreate, getOrCreate>>
a| Gets the current link:spark-sql-SparkSession.adoc[SparkSession] or creates a new one.

[source, scala]
----
getOrCreate(): SparkSession
----

| <<master, master>>
a|

[source, scala]
----
master(master: String): Builder
----

| <<withExtensions, withExtensions>>
a| Access to the <<spark-sql-SparkSessionExtensions.adoc#, SparkSessionExtensions>>

[source, scala]
----
withExtensions(f: SparkSessionExtensions => Unit): Builder
----
|===

`Builder` is available using the <<spark-sql-SparkSession.adoc#builder, builder>> object method of a SparkSession.

[source, scala]
----
import org.apache.spark.sql.SparkSession
val spark = SparkSession.builder
  .appName("My Spark Application")  // optional and will be autogenerated if not specified
  .master("local[*]")               // only for demo and testing purposes, use spark-submit instead
  .enableHiveSupport()              // self-explanatory, isn't it?
  .config("spark.sql.warehouse.dir", "target/spark-warehouse")
  .withExtensions { extensions =>
    extensions.injectResolutionRule { session =>
      ...
    }
    extensions.injectOptimizerRule { session =>
      ...
    }
  }
  .getOrCreate
----

NOTE: You can have multiple ``SparkSession``s in a single Spark application for different link:spark-sql-SparkSession.adoc#catalog[data catalogs] (through relational entities).

[[internal-registries]]
.Builder's Internal Properties (e.g. Registries, Counters and Flags)
[cols="1m,2",options="header",width="100%"]
|===
| Name
| Description

| extensions
| [[extensions]] <<spark-sql-SparkSessionExtensions.adoc#, SparkSessionExtensions>>

Used when...FIXME

| options
| [[options]]

Used when...FIXME
|===

=== [[getOrCreate]] Getting Or Creating SparkSession Instance -- `getOrCreate` Method

[source, scala]
----
getOrCreate(): SparkSession
----

`getOrCreate`...FIXME

=== [[enableHiveSupport]] Enabling Hive Support -- `enableHiveSupport` Method

[source, scala]
----
enableHiveSupport(): Builder
----

`enableHiveSupport` enables link:hive/index.adoc[Hive support] (that allows running structured queries on Hive tables, a persistent Hive metastore, support for Hive serdes and user-defined functions).

[NOTE]
====
You do *not* need any existing Hive installation to use Spark's Hive support. `SparkSession` context will automatically create `metastore_db` in the current directory of a Spark application and the directory configured by link:spark-sql-StaticSQLConf.adoc#spark.sql.warehouse.dir[spark.sql.warehouse.dir] configuration property.

Refer to link:spark-sql-SharedState.adoc[SharedState].
====

Internally, `enableHiveSupport` checks whether the <<hiveClassesArePresent, Hive classes are available>> or not. If so, `enableHiveSupport` sets link:spark-sql-StaticSQLConf.adoc#spark.sql.catalogImplementation[spark.sql.catalogImplementation] internal configuration property to `hive`. Otherwise, `enableHiveSupport` throws an `IllegalArgumentException`:

```
Unable to instantiate SparkSession with Hive support because Hive classes are not found.
```

=== [[hiveClassesArePresent]] `hiveClassesArePresent` Method

[source, scala]
----
hiveClassesArePresent: Boolean
----

`hiveClassesArePresent` loads and initializes link:hive/HiveSessionStateBuilder.adoc[org.apache.spark.sql.hive.HiveSessionStateBuilder] and `org.apache.hadoop.hive.conf.HiveConf` classes from the current classloader.

`hiveClassesArePresent` returns `true` when the initialization succeeded, and `false` otherwise (due to `ClassNotFoundException` or `NoClassDefFoundError` errors).

[NOTE]
====
`hiveClassesArePresent` is used when:

* `Builder` is requested to <<enableHiveSupport, enableHiveSupport>>

* `spark-shell` is executed
====

=== [[withExtensions]] `withExtensions` Method

[source, scala]
----
withExtensions(f: SparkSessionExtensions => Unit): Builder
----

`withExtensions` simply executes the input `f` function with the <<extensions, SparkSessionExtensions>>.

=== [[appName]] `appName` Method

[source, scala]
----
appName(name: String): Builder
----

`appName`...FIXME

=== [[config]] `config` Method

[source, scala]
----
config(conf: SparkConf): Builder
config(key: String, value: Boolean): Builder
config(key: String, value: Double): Builder
config(key: String, value: Long): Builder
config(key: String, value: String): Builder
----

`config`...FIXME

=== [[master]] `master` Method

[source, scala]
----
master(master: String): Builder
----

`master`...FIXME
