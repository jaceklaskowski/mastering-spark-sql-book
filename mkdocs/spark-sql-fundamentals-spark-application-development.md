# Fundamentals of Spark SQL Application Development

Development of a Spark SQL application requires the following steps:

. Setting up Development Environment (IntelliJ IDEA, Scala and sbt)
. Specifying Library Dependencies
. Creating <<spark-sql-SparkSession.adoc#, SparkSession>>
. <<spark-sql-DataFrameReader.adoc#, Loading Data>> from Data Sources
. Processing Data Using <<spark-sql-dataset-operators.adoc#, Dataset API>>
. <<spark-sql-DataFrameWriter.adoc#, Saving Data>> to Persistent Storage
. Deploying Spark Application to Cluster (using `spark-submit`)
